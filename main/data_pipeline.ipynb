{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3ecf773-f2ee-4aad-9a66-e57398c7e67d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8dc810-63eb-4348-b4f3-078f4b2e50f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f798ba17-79a6-486c-97e0-b20b899258c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "del gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8539ace-0125-40d0-baa3-4d3684b47d1c",
   "metadata": {},
   "source": [
    "# Build a universal one hot encoder that encodes cross-dataset category and ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09725b24-85d1-4ecb-9b34-3894255a1c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder:\n",
    "    def __init__(self, all_category_list, all_ingredient_list):\n",
    "        self.all_food_categories = all_category_list\n",
    "        self.all_food_categories.sort()\n",
    "        self.all_food_categories_integer_encoded = (\n",
    "            self.__encode_categories_to_integers()\n",
    "        )\n",
    "        self.all_ingredients = all_ingredient_list\n",
    "        self.all_ingredients.sort()\n",
    "        self.all_ingredients_integer_encoded = self.__encode_ingredients_to_integers()\n",
    "\n",
    "    def get_category_one_hot_encoding(self, category_name):\n",
    "        index = self.all_food_categories_integer_encoded[category_name]\n",
    "        assert index is not None, f\"{category_name} does not have an integer mapping\"\n",
    "        num_classes = len(self.all_food_categories)\n",
    "        return keras.utils.to_categorical(index, num_classes, dtype=\"uint8\")\n",
    "\n",
    "    def get_ingredients_one_hot_encoding(self, ingredient_list):\n",
    "        ingredient_list = list(\n",
    "            map(lambda x: self.__transform_ingredient_to_integer(x), ingredient_list)\n",
    "        )\n",
    "        multi_one_hot_layer = tf.keras.layers.CategoryEncoding(\n",
    "            num_tokens=len(self.all_ingredients), output_mode=\"multi_hot\"\n",
    "        )\n",
    "        return tf.cast(multi_one_hot_layer(ingredient_list), dtype=tf.uint8)\n",
    "\n",
    "    def __transform_ingredient_to_integer(self, ingredient_name):\n",
    "        index = self.all_ingredients_integer_encoded[ingredient_name]\n",
    "        assert index is not None, f\"{ingredient_name} does not have an integer mapping\"\n",
    "        return index\n",
    "\n",
    "    def __encode_categories_to_integers(self):\n",
    "        return {\n",
    "            category_name: index\n",
    "            for index, category_name in enumerate(self.all_food_categories)\n",
    "        }\n",
    "\n",
    "    def __encode_ingredients_to_integers(self):\n",
    "        return {\n",
    "            ingredient_name: index\n",
    "            for index, ingredient_name in enumerate(self.all_ingredients)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c256db-8606-4a26-aa9a-d78bae31270b",
   "metadata": {},
   "source": [
    "# Build dataset loaders for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef193835-66bf-4ded-b62c-d0d286157b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, image_dir, metadata_dir, dataset_name):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.metadata_dir = Path(metadata_dir)\n",
    "        self.name = dataset_name\n",
    "        self.metadata = self.load_metadata(\n",
    "            self.metadata_dir / (\"{dataset}_metadata.csv\".format(dataset=dataset_name))\n",
    "        )\n",
    "        # Default : all_files = metadata, since metadata consists of records of all files\n",
    "        self.all_files = self.metadata.copy()\n",
    "        self.all_categories = self.extract_all_categories()\n",
    "        self.all_ingredients = self.extract_all_ingredients()\n",
    "\n",
    "    def load_image_to_arr(self, path):\n",
    "        image = tf.keras.preprocessing.image.load_img(path)\n",
    "        img_tensor = tf.keras.preprocessing.image.img_to_array(image, dtype=\"uint8\")\n",
    "        img_tensor = tf.image.resize(img_tensor, (224, 224))\n",
    "        return tf.cast(img_tensor, tf.uint8)\n",
    "\n",
    "    def load_metadata(self, path):\n",
    "        metadata = pd.read_csv(path, sep=\"\\t\")\n",
    "        new_metadata = metadata.copy()\n",
    "        new_metadata[\"dataset_name\"] = self.name\n",
    "        return new_metadata\n",
    "\n",
    "    def extract_all_categories(self):\n",
    "        return self.metadata[\"Category\"].unique().tolist()\n",
    "\n",
    "    def extract_all_ingredients(self):\n",
    "        unique_ingredients = set()\n",
    "        for ingredient_list in self.metadata[\"Ingredients\"]:\n",
    "            ingredient_list = ingredient_list.split(\",\")\n",
    "            unique_ingredients.update(ingredient_list)\n",
    "        return [*unique_ingredients]\n",
    "\n",
    "    def extract_file_pointers(self):\n",
    "        dataset_name_col = self.all_files[\"dataset_name\"]\n",
    "        index_col = self.all_files.index\n",
    "        return pd.DataFrame(\n",
    "            {\"metadata_index\": index_col, \"dataset_name\": dataset_name_col}\n",
    "        )\n",
    "\n",
    "    def get_tensors(self, index, one_hot_encoder):\n",
    "        img_dir = self.image_dir\n",
    "        row = self.all_files.loc[index]\n",
    "        img_path = img_dir / row[\"Category\"] / row[\"ID/File Name\"]\n",
    "        img_tensor = self.load_image_to_arr(img_path)\n",
    "        if img_path.suffix == \".jpeg\" or img_path.suffix == \".jpg\":\n",
    "            img_tensor = tf.io.encode_jpeg(img_tensor, format=\"rgb\")\n",
    "        elif img_path.suffix == \".png\":\n",
    "            img_tensor = tf.io.encode_png(img_tensor)\n",
    "        else:\n",
    "            assert False, \"Invalid image format present\"\n",
    "        calorie_tensor = row[\"Calorie(kcal)\"]\n",
    "        carbs_tensor = row[\"Carbohydrate(g)\"]\n",
    "        protein_tensor = row[\"Protein(g)\"]\n",
    "        fat_tensor = row[\"Fat(g)\"]\n",
    "        # one_hot_category_tensor = one_hot_encoder.get_category_one_hot_encoding(\n",
    "        #     row[\"Category\"]\n",
    "        # )\n",
    "        # one_hot_ingredient_tensor = one_hot_encoder.get_ingredients_one_hot_encoding(\n",
    "        #     row[\"Ingredients\"].split(\",\")\n",
    "        # )\n",
    "        return img_tensor, {\n",
    "            \"category_output\": tf.constant(row[\"Category\"]),\n",
    "            \"calorie_output\": tf.constant(calorie_tensor),\n",
    "            \"carbs_output\": tf.constant(carbs_tensor),\n",
    "            \"protein_output\": tf.constant(protein_tensor),\n",
    "            \"fat_output\": tf.constant(fat_tensor),\n",
    "            \"ingredients_output\": tf.constant(row[\"Ingredients\"]),\n",
    "        }\n",
    "\n",
    "    def flatten_tensors(self, tensor):\n",
    "        result = []\n",
    "        img_data = tensor[0].numpy()\n",
    "        others_data = [value.numpy() for key, value in tensor[1].items()]\n",
    "        result.append(img_data)\n",
    "        result.extend(others_data)\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f876af8-e890-447a-8d1c-3f07969052ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Recipes5k(DatasetLoader):\n",
    "    def __init__(self, image_dir, metadata_dir):\n",
    "        super().__init__(image_dir, metadata_dir, \"recipes5k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0a54d2-b85c-4aac-a8b2-902ad88b6f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Nutrition5k(DatasetLoader):\n",
    "    def __init__(self, image_dir, metadata_dir):\n",
    "        super().__init__(image_dir, metadata_dir, \"nutrition5k\")\n",
    "        # Modify all_files since nutrition5k metadata only consists dish_level metadata not image_level\n",
    "        self.all_files = pd.read_csv(self.metadata_dir / \"nutrition5k_all_images.csv\")\n",
    "\n",
    "    # Override method from DatasetLoader\n",
    "    def get_tensors(self, index, one_hot_encoder):\n",
    "        img_dir = self.image_dir\n",
    "        row = self.all_files.loc[index]\n",
    "        img_path = img_dir / \"generic\" / row[\"dish_id\"] / row[\"ID/File Name\"]\n",
    "        img_tensor = self.load_image_to_arr(img_path)\n",
    "        if img_path.suffix == \".jpeg\" or img_path.suffix == \".jpg\":\n",
    "            img_tensor = tf.io.encode_jpeg(img_tensor, format=\"rgb\")\n",
    "        elif img_path.suffix == \".png\":\n",
    "            img_tensor = tf.io.encode_png(img_tensor)\n",
    "        else:\n",
    "            assert False, \"Invalid image format present\"\n",
    "        dish_metadata_row = self.metadata.loc[\n",
    "            self.metadata[\"dish_id\"] == row[\"dish_id\"]\n",
    "        ].squeeze()\n",
    "        calorie_tensor = dish_metadata_row[\"Calorie(kcal)\"]\n",
    "        carbs_tensor = dish_metadata_row[\"Carbohydrate(g)\"]\n",
    "        protein_tensor = dish_metadata_row[\"Protein(g)\"]\n",
    "        fat_tensor = dish_metadata_row[\"Fat(g)\"]\n",
    "        # one_hot_category_tensor = one_hot_encoder.get_category_one_hot_encoding(\n",
    "        #     dish_metadata_row[\"Category\"]\n",
    "        # )\n",
    "        # one_hot_ingredient_tensor = one_hot_encoder.get_ingredients_one_hot_encoding(\n",
    "        #     dish_metadata_row[\"Ingredients\"].split(\",\")\n",
    "        # )\n",
    "        return img_tensor, {\n",
    "            \"category_output\": tf.constant(dish_metadata_row[\"Category\"]),\n",
    "            \"calorie_output\": tf.constant(calorie_tensor),\n",
    "            \"carbs_output\": tf.constant(carbs_tensor),\n",
    "            \"protein_output\": tf.constant(protein_tensor),\n",
    "            \"fat_output\": tf.constant(fat_tensor),\n",
    "            \"ingredients_output\": tf.constant(dish_metadata_row[\"Ingredients\"]),\n",
    "        }\n",
    "\n",
    "    # Overrding the method from DatasetLoader\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ff5ec7-bf8c-472f-9f46-10c3042dc391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Food101(DatasetLoader):\n",
    "    def __init__(self, image_dir, metadata_dir):\n",
    "        super().__init__(image_dir, metadata_dir, \"food101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb374be-7cf9-4042-be86-473a60a8f553",
   "metadata": {},
   "source": [
    "# Initializing one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc3c605-ac9d-423b-ae0d-154d1641cd39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all the categories and ingredients from all datasets\n",
    "\n",
    "# Initialize dataset loader without one-hot encoder to get all unique category and ingredients from each dataset\n",
    "RECIPES5K = Recipes5k(\n",
    "    image_dir=\"../Food Datasets/final-dataset/images\",\n",
    "    metadata_dir=\"../Food Datasets/final-dataset/metadata\",\n",
    ")\n",
    "NUTRITION5K = Nutrition5k(\n",
    "    image_dir=\"../Food Datasets/final-dataset/images\",\n",
    "    metadata_dir=\"../Food Datasets/final-dataset/metadata\",\n",
    ")\n",
    "FOOD101 = Food101(\n",
    "    image_dir=\"../Food Datasets/final-dataset/images\",\n",
    "    metadata_dir=\"../Food Datasets/final-dataset/metadata\",\n",
    ")\n",
    "\n",
    "DATASETS = [RECIPES5K, NUTRITION5K, FOOD101]\n",
    "DATASETS_NAME = [x.name for x in DATASETS]\n",
    "\n",
    "\n",
    "def create_one_hot_encoder(datasets):\n",
    "    all_categories = []\n",
    "    all_ingredients = []\n",
    "    for x in datasets:\n",
    "        all_categories.extend(x.all_categories)\n",
    "        all_ingredients.extend(x.all_ingredients)\n",
    "    all_categories = set(all_categories)\n",
    "    all_ingredients = set(all_ingredients)\n",
    "    return OneHotEncoder([*all_categories], [*all_ingredients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08bb542-3d60-4033-8216-8a316cdeebf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONE_HOT_ENCODER = create_one_hot_encoder(DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ad655-5f17-43b3-b75e-e71aa19c9f88",
   "metadata": {},
   "source": [
    "# Building dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe84dc4a-1fb4-48a1-9a2c-29b29ebbfaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_cardinality(datasets):\n",
    "    dataset_samples = [len(x) for x in datasets]\n",
    "    return sum(dataset_samples)\n",
    "\n",
    "\n",
    "def get_file_data(index, dataset_index):\n",
    "    # index = index.numpy()\n",
    "    # dataset_index = dataset_index.numpy()\n",
    "    target_dataset = DATASETS[dataset_index]\n",
    "    return target_dataset.flatten_tensors(\n",
    "        target_dataset.get_tensors(index, ONE_HOT_ENCODER)\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_file_pointers(index, dataset_index):\n",
    "    result = tf.py_function(\n",
    "        get_file_data,\n",
    "        [index, dataset_index],\n",
    "        [\n",
    "            tf.string,\n",
    "            tf.string,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.string,\n",
    "        ],\n",
    "    )\n",
    "    return tf.data.Dataset.from_tensors(tuple(result))\n",
    "\n",
    "\n",
    "def build_data_pipeline(datasets, sample_size=None):\n",
    "    if sample_size is None:\n",
    "        sample_size = [1.0] * len(datasets)\n",
    "    assert len(sample_size) == len(\n",
    "        datasets\n",
    "    ), \"Illegal array of sample sizes provided. Number of sample size does not match number of datasets\"\n",
    "    file_pointers = [\n",
    "        x.extract_file_pointers().sample(frac=s) for x, s in zip(datasets, sample_size)\n",
    "    ]\n",
    "    all_file_pointers = pd.concat(file_pointers).sample(frac=1, random_state=999)\n",
    "    print(f\"Total samples : {len(all_file_pointers)}\")\n",
    "\n",
    "    all_file_pointers[\"dataset_name\"] = all_file_pointers[\"dataset_name\"].apply(\n",
    "        lambda x: DATASETS_NAME.index(x)\n",
    "    )\n",
    "\n",
    "    final_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            all_file_pointers[\"metadata_index\"].tolist(),\n",
    "            all_file_pointers[\"dataset_name\"].tolist(),\n",
    "        )\n",
    "    )\n",
    "    # final_dataset = final_dataset.interleave(\n",
    "    #     lambda index, name: transform_file_pointers(index, name),\n",
    "    #     num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    # ).prefetch(tf.data.AUTOTUNE)\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f30aff-2b6e-48f9-9e2e-977224538d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples : 4826\n",
      "Total samples : 101000\n",
      "Total samples : 271407\n"
     ]
    }
   ],
   "source": [
    "recipes5k_dataset = build_data_pipeline([RECIPES5K])\n",
    "food101_dataset = build_data_pipeline([FOOD101])\n",
    "nutrition5k_dataset = build_data_pipeline([NUTRITION5K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2910985d-06c6-47bb-979d-bedb7b7d1a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(), dtype=int32, numpy=865>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=0>),\n",
       " (<tf.Tensor: shape=(), dtype=int32, numpy=4413>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(recipes5k_dataset.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3f017-46fc-4bdc-8eda-8d7aa69b4097",
   "metadata": {},
   "source": [
    "## Serializing Data Pipeline to TFRecord with TFDS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8175168-8a97-4ea6-a457-279dbf582c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DICTIONARY = tfds.features.FeaturesDict(\n",
    "    {\n",
    "        \"image_raw\": tfds.features.Image(\n",
    "            shape=(224, 224, 3), doc=\"Raw bytes of food images encoded with tf.io\"\n",
    "        ),\n",
    "        \"category\": tfds.features.Scalar(dtype=tf.string, doc=\"Category label\"),\n",
    "        \"calorie\": tfds.features.Scalar(\n",
    "            dtype=tf.float32, doc=\"Calorie of the food per gram\"\n",
    "        ),\n",
    "        \"carbs\": tfds.features.Scalar(\n",
    "            dtype=tf.float32, doc=\"Carbs of the food per gram\"\n",
    "        ),\n",
    "        \"protein\": tfds.features.Scalar(\n",
    "            dtype=tf.float32, doc=\"Protein of the food per gram\"\n",
    "        ),\n",
    "        \"fat\": tfds.features.Scalar(dtype=tf.float32, doc=\"Fat of the food per gram\"),\n",
    "        \"ingredients\": tfds.features.Scalar(\n",
    "            dtype=tf.string, doc=\"Ingredients of food separated with comma\"\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2330f2-ee56-40d7-9404-a4025452bbcf",
   "metadata": {},
   "source": [
    "### Shard and write to TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5224e67b-b845-4493-9eee-1a39633d31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_and_write(dataset, num_shards, path, dataset_name):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "\n",
    "    sharded_template_generator = tfds.core.ShardedFileTemplate(\n",
    "        data_dir=path.as_posix(),\n",
    "        template=\"{DATASET}-{SPLIT}-{SHARD_X_OF_Y}.{FILEFORMAT}\",\n",
    "        dataset_name=dataset_name,\n",
    "        filetype_suffix=\"tfrecord\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    shard_length = []\n",
    "    sharded_filepaths = sharded_template_generator.sharded_filepaths(num_shards)\n",
    "    for i in range(num_shards):\n",
    "        current_shard = dataset.shard(num_shards, i)\n",
    "        with tf.io.TFRecordWriter(sharded_filepaths[i].as_posix()) as writer:\n",
    "            length = 0\n",
    "            for record in current_shard.as_numpy_iterator():\n",
    "                data = get_file_data(record[0], record[1])\n",
    "                example = {\n",
    "                    \"image_raw\": data[0],\n",
    "                    \"category\": data[1],\n",
    "                    \"calorie\": data[2],\n",
    "                    \"carbs\": data[3],\n",
    "                    \"protein\": data[4],\n",
    "                    \"fat\": data[5],\n",
    "                    \"ingredients\": data[6],\n",
    "                }\n",
    "                example_bytes = FEATURE_DICTIONARY.serialize_example(example)\n",
    "                writer.write(example_bytes)\n",
    "                length += 1\n",
    "            shard_length.append(length)\n",
    "    split_info = [\n",
    "        tfds.core.SplitInfo(\n",
    "            name=\"train\",\n",
    "            shard_lengths=shard_length,\n",
    "            num_bytes=0,\n",
    "            filename_template=sharded_template_generator,\n",
    "        )\n",
    "    ]\n",
    "    tfds.folder_dataset.write_metadata(\n",
    "        data_dir=path.as_posix(),\n",
    "        features=FEATURE_DICTIONARY,\n",
    "        filename_template=\"{DATASET}-{SPLIT}-{SHARD_X_OF_Y}.{FILEFORMAT}\",\n",
    "        split_infos=split_info,\n",
    "    )\n",
    "    return shard_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3fbc0c2-c3ba-4979-81fc-4f7fa20d4f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shard_length = shard_and_write(\n",
    "    recipes5k_dataset,\n",
    "    10,\n",
    "    f\"../Food Datasets/final-dataset/tfrecord/{RECIPES5K.name}/1.0.0\",\n",
    "    RECIPES5K.name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a37df5-2dc7-454f-b78c-c9c5cc773843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata written. Testing by reading first example. Set check_data=False to skip.\n"
     ]
    }
   ],
   "source": [
    "shard_length2 = shard_and_write(\n",
    "    food101_dataset,\n",
    "    30,\n",
    "    f\"../Food Datasets/final-dataset/tfrecord/{FOOD101.name}/1.0.1\",\n",
    "    FOOD101.name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580157ab-38c1-4725-82ce-2c15da26b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_length3 = shard_and_write(\n",
    "    nutrition5k_dataset,\n",
    "    100,\n",
    "    f\"../Food Datasets/final_dataset/tfrecord/{NUTRITION5K.name}/1.0.0\",\n",
    "    NUTRITION5K.name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d605606-2412-45d8-a609-1818a9db999e",
   "metadata": {},
   "source": [
    "# Exported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d8ff3-8289-46a2-b34c-8c83e8d54cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED = {\"datasets\": DATASETS, \"one_hot_encoder\": ONE_HOT_ENCODER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd129b6-b1b5-489f-84f6-18a10968bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED = SimpleNamespace(**EXPORTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478aee1a-364c-41c7-aa62-d8073a9c9ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c07aee62-b540-437e-8ce1-e4843d6a4e96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28222fc8-4370-4c36-92c2-0e3b2da0ca29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_recipe5k_metadata():\n",
    "    directory = (\n",
    "        Path(\"../Food Datasets/final-dataset\") / \"metadata\" / \"recipes5k_metadata.csv\"\n",
    "    )\n",
    "    return pd.read_csv(directory, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb0b37-b9a6-468d-af10-d6c8d586cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_recipe5k_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c716989-17a8-4dec-b39b-0da69a651bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac060dd-2bd2-436d-95e0-cfca89279988",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipes5k = Recipes5k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070186ec-97e1-4e0b-ba50-bf5ed098d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_recipes5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a5da6-1092-4b95-a3d1-18b611e35313",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_func = test_recipes5k.generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25e5e3-dabd-4919-93d0-11bfbceee04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_recipes5k.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe0cbe-5471-44f9-aaea-10e27e46892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(RECIPES5K.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609ed77-693c-48e7-9e47-7ee7245d66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d4e65-82b9-46b1-ab3f-d34c024446b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = test_model.build_and_compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d8a3d-53d9-4ae1-88b3-b88181b8d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d7b51-a6d0-4816-8840-086fa718ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.fit(\n",
    "    x=test_recipes5k.training_dataset,\n",
    "    epochs=1,\n",
    "    verbose=1,\n",
    "    validation_data=test_recipes5k.validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63930dc8-5c6c-4f8a-9242-1a5e24d46cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.constant([[[1, 2, 3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9608f3-5f40-42bf-ba5e-190e7974bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2e37c-0a32-4163-9614-efeec8704533",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6bcc84-7c59-454b-adcf-eeaf96eaa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipes5k.training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c37e2-0a30-40f3-b0d3-820bc68cad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipes5k.validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a044e7-d57f-4f94-9c47-0fbbb209631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "for x in test_recipes5k.training_dataset:\n",
    "    row += 1\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49f7bb-0cf2-48ec-a22c-71661efed140",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38015910-b177-41ef-9d97-9138a7f79f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check training dataset (first 70%)\n",
    "training_rows = int(len(RECIPES5K) * 0.7)\n",
    "training_data = RECIPES5K.metadata.iloc[:training_rows]\n",
    "validation_data = RECIPES5K.metadata.iloc[training_rows:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc278e-9834-4caf-bf7d-fe1d00d59b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e9d02b-37e8-4e1a-a7d9-3c1bce59875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check categories of training\n",
    "category_count = {x: 0 for x in RECIPES5K.extract_all_categories()}\n",
    "for x in training_data[\"Category\"].tolist():\n",
    "    category_count[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be353e8-b66d-4456-8f6f-7246e7782330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f7922b-b3a9-4e16-a698-f5398659f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_count = {x: 0 for x in RECIPES5K.extract_all_categories()}\n",
    "for x in validation_data[\"Category\"].tolist():\n",
    "    validation_count[x] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a575a-760a-44f1-8758-693df2dfe452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(validation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a91346-0c88-4f22-b8a2-e50661e53a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(RECIPES5K.training_dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ffe35-0ea4-4891-820c-f16c6b1f5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2_convolution_layers = keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80302fab-a85a-4b95-961f-8836504cb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2_convolution_layers.layers[-1].output_shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc2972-a4ab-4e3e-96bb-6146162d4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2_convolution_layers.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf3a8a-ecf2-44df-b000-3659444cdb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = RegionWise_FoodNet_MobileNetv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b973ad-99d2-439e-94eb-50f68890fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = test_model.mobilenetv2_convolution_block(test_model.input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8dc0dc-5618-4f7a-a9a8-1519568e7852",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = test_model.ingredient_classifier(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296aac3-85c5-40d7-81b4-e91a6328d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = keras.Model(test_model.input_layer, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8facc9a-572e-45e8-8d68-4d82c0a0579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02b98e-a001-4f7d-af47-1ea157710730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_model.ingredient_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c0592-bcaa-4c34-aa19-714d1f5c51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = keras.layers.concatenate(\n",
    "    [\n",
    "        tf.constant([1, 0, 1]),\n",
    "        tf.constant([0, 0, 2]),\n",
    "        tf.constant([1, 0, 1]),\n",
    "        tf.constant([5, 0, 1]),\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e40df7-51bb-4c66-9c6d-08381e429260",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.reshape(x, (1, 4, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413904e5-1de8-459c-b301-361ba1ce2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.MaxPooling1D()(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5aa75d-704b-40d3-93dd-eecb483735b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.GlobalMaxPooling1D()(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7caab9-6473-4440-a04b-6c70bfd922f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.data.Dataset.range(300000)\n",
    "df = df.shuffle(200000)\n",
    "list(df.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f6c63a-a8df-439e-9742-26bc61a02b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Overriding the method from DatasetLoader\n",
    "#     def generate_dataset(self):\n",
    "#         img_dir = self.dir_path / \"images\"\n",
    "#         for index, row in self.metadata.iterrows():\n",
    "#             dish_dir = img_dir / row[\"Category\"] / row[\"dish_id\"]\n",
    "#             for img_path in dish_dir.iterdir():\n",
    "#                 assert (\n",
    "#                     img_path.suffix == \".jpeg\" or img_path.suffix == \".png\"\n",
    "#                 ), f\"{img_path} is not an expected image file\"\n",
    "#                 img_tensor = self.load_image_to_arr(img_path)\n",
    "#                 calorie_tensor = row[\"Calorie(kcal)\"]\n",
    "#                 carbs_tensor = row[\"Carbohydrate(g)\"]\n",
    "#                 protein_tensor = row[\"Protein(g)\"]\n",
    "#                 fat_tensor = row[\"Fat(g)\"]\n",
    "#                 one_hot_category_tensor = (\n",
    "#                     self.one_hot_encoder.get_category_one_hot_encoding(row[\"Category\"])\n",
    "#                 )\n",
    "#                 one_hot_ingredient_tensor = (\n",
    "#                     self.one_hot_encoder.get_ingredients_one_hot_encoding(\n",
    "#                         row[\"Ingredients\"].split(\",\")\n",
    "#                     )\n",
    "#                 )\n",
    "#                 yield tf.constant(img_tensor), {\n",
    "#                     \"category_output\": tf.constant(one_hot_category_tensor),\n",
    "#                     \"calorie_output\": tf.constant(calorie_tensor),\n",
    "#                     \"carbs_output\": tf.constant(carbs_tensor),\n",
    "#                     \"protein_output\": tf.constant(protein_tensor),\n",
    "#                     \"fat_output\": tf.constant(fat_tensor),\n",
    "#                     \"ingredients_output\": one_hot_ingredient_tensor,\n",
    "#                 }\n",
    "\n",
    "#     # Overriding the method from DatasetLoader\n",
    "#     def get_dataset(self):\n",
    "#         dataset = tf.data.Dataset.from_generator(\n",
    "#             self.generate_dataset,\n",
    "#             output_signature=(\n",
    "#                 tf.TensorSpec(shape=(224, 224, 3), dtype=tf.dtypes.float32),\n",
    "#                 {\n",
    "#                     \"category_output\": tf.TensorSpec(\n",
    "#                         shape=(len(self.one_hot_encoder.all_food_categories)),\n",
    "#                         dtype=tf.dtypes.float32,\n",
    "#                     ),\n",
    "#                     \"calorie_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "#                     \"carbs_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "#                     \"protein_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "#                     \"fat_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "#                     \"ingredients_output\": tf.TensorSpec(\n",
    "#                         shape=(len(self.one_hot_encoder.all_ingredients)),\n",
    "#                         dtype=tf.dtypes.float32,\n",
    "#                     ),\n",
    "#                 },\n",
    "#             ),\n",
    "#         )\n",
    "#         return dataset.shuffle(len(self.metadata) * 30, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3839ab61-c98f-4d25-ac33-0af20777979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.constant(\"sting\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd26651-1725-4d23-a6d6-c6a35288c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf9314-3fc9-4dc6-b907-994fb0ec53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_index -> range(total_index) -> shuffle -> interleave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045e836-56f1-4397-b7d9-e35d980792e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensors((1, {\"a\": 1, \"b\": 2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37e58e-8950-4da7-8b5f-6e2e109ffdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95639ab2-a501-4368-a2b8-69e2f738744a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e64d6-7bdc-4d6b-8620-530f160191ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_cardinality(datasets):\n",
    "    dataset_samples = [len(x) for x in datasets]\n",
    "    return sum(dataset_samples)\n",
    "\n",
    "\n",
    "def get_file_data(index, dataset_index):\n",
    "    index = index.numpy()\n",
    "    dataset_index = dataset_index.numpy()\n",
    "    target_dataset = DATASETS[dataset_index]\n",
    "    return target_dataset.flatten_tensors(\n",
    "        target_dataset.get_tensors(index, ONE_HOT_ENCODER)\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_file_pointers(index, dataset_index):\n",
    "    result = tf.py_function(\n",
    "        get_file_data,\n",
    "        [index, dataset_index],\n",
    "        [\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "            tf.float32,\n",
    "        ],\n",
    "    )\n",
    "    return result[0], {\n",
    "        \"category_output\": result[1],\n",
    "        \"calorie_output\": result[2],\n",
    "        \"carbs_output\": result[3],\n",
    "        \"protein_output\": result[4],\n",
    "        \"fat_output\": result[5],\n",
    "        \"ingredients_output\": result[6],\n",
    "    }\n",
    "\n",
    "\n",
    "def build_data_pipeline(datasets, sample_size=None, training_split=0.7, batch_size=32):\n",
    "    if sample_size is None:\n",
    "        sample_size = [1.0] * len(datasets)\n",
    "    assert len(sample_size) == len(\n",
    "        datasets\n",
    "    ), \"Illegal array of sample sizes provided. Number of sample size does not match number of datasets\"\n",
    "    file_pointers = [\n",
    "        x.extract_file_pointers().sample(frac=s) for x, s in zip(datasets, sample_size)\n",
    "    ]\n",
    "    all_file_pointers = pd.concat(file_pointers).sample(frac=1)\n",
    "    print(f\"Total samples : {len(all_file_pointers)}\")\n",
    "\n",
    "    all_file_pointers[\"dataset_name\"] = all_file_pointers[\"dataset_name\"].apply(\n",
    "        lambda x: DATASETS_NAME.index(x)\n",
    "    )\n",
    "    training_size = int(len(all_file_pointers) * training_split)\n",
    "    # training_dataset = (\n",
    "    #     tf.data.Dataset.from_generator(\n",
    "    #         get_generator(all_file_pointers.iloc[:training_size]),\n",
    "    #         output_signature=(\n",
    "    #             tf.TensorSpec(shape=(224, 224, 3), dtype=tf.dtypes.float32),\n",
    "    #             {\n",
    "    #                 \"category_output\": tf.TensorSpec(\n",
    "    #                     shape=(len(ONE_HOT_ENCODER.all_food_categories)),\n",
    "    #                     dtype=tf.dtypes.float32,\n",
    "    #                 ),\n",
    "    #                 \"calorie_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"carbs_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"protein_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"fat_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"ingredients_output\": tf.TensorSpec(\n",
    "    #                     shape=(len(ONE_HOT_ENCODER.all_ingredients)),\n",
    "    #                     dtype=tf.dtypes.float32,\n",
    "    #                 ),\n",
    "    #             },\n",
    "    #         ),\n",
    "    #     )\n",
    "    #     .cache(filename=\"./dataset_cache/train\")\n",
    "    #     .batch(batch_size)\n",
    "    #     .prefetch(tf.data.AUTOTUNE)\n",
    "    # )\n",
    "    # validation_dataset = (\n",
    "    #     tf.data.Dataset.from_generator(\n",
    "    #         get_generator(all_file_pointers.iloc[training_size:]),\n",
    "    #         output_signature=(\n",
    "    #             tf.TensorSpec(shape=(224, 224, 3), dtype=tf.dtypes.float32),\n",
    "    #             {\n",
    "    #                 \"category_output\": tf.TensorSpec(\n",
    "    #                     shape=(len(ONE_HOT_ENCODER.all_food_categories)),\n",
    "    #                     dtype=tf.dtypes.float32,\n",
    "    #                 ),\n",
    "    #                 \"calorie_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"carbs_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"protein_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"fat_output\": tf.TensorSpec(shape=(), dtype=tf.dtypes.float32),\n",
    "    #                 \"ingredients_output\": tf.TensorSpec(\n",
    "    #                     shape=(len(ONE_HOT_ENCODER.all_ingredients)),\n",
    "    #                     dtype=tf.dtypes.float32,\n",
    "    #                 ),\n",
    "    #             },\n",
    "    #         ),\n",
    "    #     )\n",
    "    #     .cache(filename=\"./dataset_cache/validation\")\n",
    "    #     .batch(batch_size)\n",
    "    #     .prefetch(tf.data.AUTOTUNE)\n",
    "    # )\n",
    "\n",
    "    final_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            all_file_pointers[\"metadata_index\"].tolist(),\n",
    "            all_file_pointers[\"dataset_name\"].tolist(),\n",
    "        )\n",
    "    )\n",
    "    training_dataset = (\n",
    "        final_dataset.take(training_size)\n",
    "        .map(\n",
    "            lambda index, name: transform_file_pointers(index, name),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        .cache(filename=\"./dataset_cache/train\")\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    validation_dataset = (\n",
    "        final_dataset.skip(training_size)\n",
    "        .map(\n",
    "            lambda index, name: transform_file_pointers(index, name),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        )\n",
    "        .cache(filename=\"./dataset_cache/validation\")\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    print(f\"Training size : {training_size}\")\n",
    "    print(f\"Validation size : {len(all_file_pointers)-training_size}\")\n",
    "    return training_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e408d-383c-43de-b295-d0d9562e4444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "backup_dir = Path(f\"./temp/backup/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5381a-f9a4-4935-9520-9c9d219df6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(backup_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26698772-2b9a-467f-a719-1d799cf04f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharded_name_generator = tfds.core.ShardedFileTemplate(\n",
    "    data_dir=f\"../Food Datasets/final-dataset/tfds/food101/1.0.0\",\n",
    "    template=\"{DATASET}-{SHARD_X_OF_Y}.{FILEFORMAT}\",\n",
    "    dataset_name=\"food101\",\n",
    "    filetype_suffix=\"tfrecord\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "347e7b66-eff6-4fbb-8e6c-b4ea553f6eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Food Datasets/final-dataset/tfds/food101/1.0.0/food101-00000-of-00010.tfrecord'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharded_name_generator.sharded_filepaths(10)[0].as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6492e0f-92b3-47f8-a9a5-fe1d5d56ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY38-TF28-GPU",
   "language": "python",
   "name": "py38-tf28-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
