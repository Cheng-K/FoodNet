{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7de337-b4bc-45bb-9418-7c68c268e03a",
   "metadata": {},
   "source": [
    "**Author :  Ong Cheng Kei TP055620** <br>\n",
    "**Description :**\n",
    "<br>This file contains code to build the serialize Recipes5k, Nutrition5k, Food101 into tfrecord files for better efficiency in model training.<br>\n",
    "This file is also a prerequisite for running the *build_model.ipynb* as it creates the one hot encoder needed to encode the categorical data<br> \n",
    "The output will be a set of tfrecord files for each dataset stored in *../Food Datasets/final-dataset/tfrecord*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ecf773-f2ee-4aad-9a66-e57398c7e67d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8dc810-63eb-4348-b4f3-078f4b2e50f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f798ba17-79a6-486c-97e0-b20b899258c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "del gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8539ace-0125-40d0-baa3-4d3684b47d1c",
   "metadata": {},
   "source": [
    "# Build a universal one hot encoder that encodes cross-dataset category and ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09725b24-85d1-4ecb-9b34-3894255a1c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OneHotEncoder:\n",
    "    def __init__(self, all_category_list, all_ingredient_list):\n",
    "        self.all_food_categories = all_category_list\n",
    "        self.all_food_categories.sort()\n",
    "        self.all_food_categories_integer_encoded = (\n",
    "            self.__encode_categories_to_integers()\n",
    "        )\n",
    "        self.all_ingredients = all_ingredient_list\n",
    "        self.all_ingredients.sort()\n",
    "        self.all_ingredients_integer_encoded = self.__encode_ingredients_to_integers()\n",
    "\n",
    "    def get_category_one_hot_encoding(self, category_name):\n",
    "        index = self.all_food_categories_integer_encoded[category_name]\n",
    "        assert index is not None, f\"{category_name} does not have an integer mapping\"\n",
    "        num_classes = len(self.all_food_categories)\n",
    "        return keras.utils.to_categorical(index, num_classes, dtype=\"uint8\")\n",
    "\n",
    "    def get_ingredients_one_hot_encoding(self, ingredient_list):\n",
    "        ingredient_list = list(\n",
    "            map(lambda x: self.__transform_ingredient_to_integer(x), ingredient_list)\n",
    "        )\n",
    "        multi_one_hot_layer = tf.keras.layers.CategoryEncoding(\n",
    "            num_tokens=len(self.all_ingredients), output_mode=\"multi_hot\"\n",
    "        )\n",
    "        return tf.cast(multi_one_hot_layer(ingredient_list), dtype=tf.uint8)\n",
    "\n",
    "    def __transform_ingredient_to_integer(self, ingredient_name):\n",
    "        index = self.all_ingredients_integer_encoded[ingredient_name]\n",
    "        assert index is not None, f\"{ingredient_name} does not have an integer mapping\"\n",
    "        return index\n",
    "\n",
    "    def __encode_categories_to_integers(self):\n",
    "        return {\n",
    "            category_name: index\n",
    "            for index, category_name in enumerate(self.all_food_categories)\n",
    "        }\n",
    "\n",
    "    def __encode_ingredients_to_integers(self):\n",
    "        return {\n",
    "            ingredient_name: index\n",
    "            for index, ingredient_name in enumerate(self.all_ingredients)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c256db-8606-4a26-aa9a-d78bae31270b",
   "metadata": {},
   "source": [
    "# Build dataset loaders for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef193835-66bf-4ded-b62c-d0d286157b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, image_dir, metadata_dir, dataset_name):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.metadata_dir = Path(metadata_dir)\n",
    "        self.name = dataset_name\n",
    "        self.metadata = self.load_metadata(\n",
    "            self.metadata_dir / (\"{dataset}_metadata.csv\".format(dataset=dataset_name))\n",
    "        )\n",
    "        # Default : all_files = metadata, since metadata consists of records of all files\n",
    "        self.all_files = self.metadata.copy()\n",
    "        self.all_categories = self.extract_all_categories()\n",
    "        self.all_ingredients = self.extract_all_ingredients()\n",
    "\n",
    "    def load_image_to_arr(self, path):\n",
    "        image = tf.keras.preprocessing.image.load_img(path)\n",
    "        img_tensor = tf.keras.preprocessing.image.img_to_array(image, dtype=\"uint8\")\n",
    "        img_tensor = tf.image.resize(img_tensor, (224, 224))\n",
    "        return tf.cast(img_tensor, tf.uint8)\n",
    "\n",
    "    def load_metadata(self, path):\n",
    "        metadata = pd.read_csv(path, sep=\"\\t\")\n",
    "        new_metadata = metadata.copy()\n",
    "        new_metadata[\"dataset_name\"] = self.name\n",
    "        return new_metadata\n",
    "\n",
    "    def extract_all_categories(self):\n",
    "        return self.metadata[\"Category\"].unique().tolist()\n",
    "\n",
    "    def extract_all_ingredients(self):\n",
    "        unique_ingredients = set()\n",
    "        for ingredient_list in self.metadata[\"Ingredients\"]:\n",
    "            ingredient_list = ingredient_list.split(\",\")\n",
    "            unique_ingredients.update(ingredient_list)\n",
    "        return [*unique_ingredients]\n",
    "\n",
    "    def extract_file_pointers(self):\n",
    "        dataset_name_col = self.all_files[\"dataset_name\"]\n",
    "        index_col = self.all_files.index\n",
    "        return pd.DataFrame(\n",
    "            {\"metadata_index\": index_col, \"dataset_name\": dataset_name_col}\n",
    "        )\n",
    "\n",
    "    def get_tensors(self, index):\n",
    "        img_dir = self.image_dir\n",
    "        row = self.all_files.loc[index]\n",
    "        img_path = img_dir / row[\"Category\"] / row[\"ID/File Name\"]\n",
    "        img_tensor = self.load_image_to_arr(img_path)\n",
    "        if img_path.suffix == \".jpeg\" or img_path.suffix == \".jpg\":\n",
    "            img_tensor = tf.io.encode_jpeg(img_tensor, format=\"rgb\")\n",
    "        elif img_path.suffix == \".png\":\n",
    "            img_tensor = tf.io.encode_png(img_tensor)\n",
    "        else:\n",
    "            assert False, \"Invalid image format present\"\n",
    "        calorie_tensor = row[\"Calorie(kcal)\"]\n",
    "        carbs_tensor = row[\"Carbohydrate(g)\"]\n",
    "        protein_tensor = row[\"Protein(g)\"]\n",
    "        fat_tensor = row[\"Fat(g)\"]\n",
    "        return img_tensor, {\n",
    "            \"category_output\": tf.constant(row[\"Category\"]),\n",
    "            \"calorie_output\": tf.constant(calorie_tensor),\n",
    "            \"carbs_output\": tf.constant(carbs_tensor),\n",
    "            \"protein_output\": tf.constant(protein_tensor),\n",
    "            \"fat_output\": tf.constant(fat_tensor),\n",
    "            \"ingredients_output\": tf.constant(row[\"Ingredients\"]),\n",
    "        }\n",
    "\n",
    "    def flatten_tensors(self, tensor):\n",
    "        result = []\n",
    "        img_data = tensor[0].numpy()\n",
    "        others_data = [value.numpy() for key, value in tensor[1].items()]\n",
    "        result.append(img_data)\n",
    "        result.extend(others_data)\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f876af8-e890-447a-8d1c-3f07969052ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Recipes5k(DatasetLoader):\n",
    "    def __init__(self, image_dir, metadata_dir):\n",
    "        super().__init__(image_dir, metadata_dir, \"recipes5k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a0a54d2-b85c-4aac-a8b2-902ad88b6f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Nutrition5k(DatasetLoader):\n",
    "    def __init__(self, image_dir, metadata_dir):\n",
    "        super().__init__(image_dir, metadata_dir, \"nutrition5k\")\n",
    "        # Modify all_files since nutrition5k metadata only consists dish_level metadata not image_level\n",
    "        self.all_files = pd.read_csv(self.metadata_dir / \"nutrition5k_all_images.csv\")\n",
    "\n",
    "    # Override method from DatasetLoader\n",
    "    def get_tensors(self, index):\n",
    "        img_dir = self.image_dir\n",
    "        row = self.all_files.loc[index]\n",
    "        img_path = img_dir / \"generic\" / row[\"dish_id\"] / row[\"ID/File Name\"]\n",
    "        img_tensor = self.load_image_to_arr(img_path)\n",
    "        if img_path.suffix == \".jpeg\" or img_path.suffix == \".jpg\":\n",
    "            img_tensor = tf.io.encode_jpeg(img_tensor, format=\"rgb\")\n",
    "        elif img_path.suffix == \".png\":\n",
    "            img_tensor = tf.io.encode_png(img_tensor)\n",
    "        else:\n",
    "            assert False, \"Invalid image format present\"\n",
    "        dish_metadata_row = self.metadata.loc[\n",
    "            self.metadata[\"dish_id\"] == row[\"dish_id\"]\n",
    "        ].squeeze()\n",
    "        calorie_tensor = dish_metadata_row[\"Calorie(kcal)\"]\n",
    "        carbs_tensor = dish_metadata_row[\"Carbohydrate(g)\"]\n",
    "        protein_tensor = dish_metadata_row[\"Protein(g)\"]\n",
    "        fat_tensor = dish_metadata_row[\"Fat(g)\"]\n",
    "        return img_tensor, {\n",
    "            \"category_output\": tf.constant(dish_metadata_row[\"Category\"]),\n",
    "            \"calorie_output\": tf.constant(calorie_tensor),\n",
    "            \"carbs_output\": tf.constant(carbs_tensor),\n",
    "            \"protein_output\": tf.constant(protein_tensor),\n",
    "            \"fat_output\": tf.constant(fat_tensor),\n",
    "            \"ingredients_output\": tf.constant(dish_metadata_row[\"Ingredients\"]),\n",
    "        }\n",
    "\n",
    "    # Overrding the method from DatasetLoader\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ff5ec7-bf8c-472f-9f46-10c3042dc391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Food101(DatasetLoader):\n",
    "    def __init__(self, image_dir, metadata_dir):\n",
    "        super().__init__(image_dir, metadata_dir, \"food101\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb374be-7cf9-4042-be86-473a60a8f553",
   "metadata": {},
   "source": [
    "# Initializing one hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc3c605-ac9d-423b-ae0d-154d1641cd39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all the categories and ingredients from all datasets\n",
    "\n",
    "# Initialize dataset loader without one-hot encoder to get all unique category and ingredients from each dataset\n",
    "RECIPES5K = Recipes5k(\n",
    "    image_dir=\"../Food Datasets/final-dataset/images\",\n",
    "    metadata_dir=\"../Food Datasets/final-dataset/metadata\",\n",
    ")\n",
    "NUTRITION5K = Nutrition5k(\n",
    "    image_dir=\"../Food Datasets/final-dataset/images\",\n",
    "    metadata_dir=\"../Food Datasets/final-dataset/metadata\",\n",
    ")\n",
    "FOOD101 = Food101(\n",
    "    image_dir=\"../Food Datasets/final-dataset/images\",\n",
    "    metadata_dir=\"../Food Datasets/final-dataset/metadata\",\n",
    ")\n",
    "\n",
    "DATASETS = [RECIPES5K, NUTRITION5K, FOOD101]\n",
    "DATASETS_NAME = [x.name for x in DATASETS]\n",
    "\n",
    "\n",
    "def create_one_hot_encoder(datasets):\n",
    "    all_categories = []\n",
    "    all_ingredients = []\n",
    "    for x in datasets:\n",
    "        all_categories.extend(x.all_categories)\n",
    "        all_ingredients.extend(x.all_ingredients)\n",
    "    all_categories = set(all_categories)\n",
    "    all_ingredients = set(all_ingredients)\n",
    "    return OneHotEncoder([*all_categories], [*all_ingredients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08bb542-3d60-4033-8216-8a316cdeebf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONE_HOT_ENCODER = create_one_hot_encoder(DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ad655-5f17-43b3-b75e-e71aa19c9f88",
   "metadata": {},
   "source": [
    "# Building data pipeline that streams the file index and dataset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe84dc4a-1fb4-48a1-9a2c-29b29ebbfaa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_data(index, dataset_index):\n",
    "    target_dataset = DATASETS[dataset_index]\n",
    "    return target_dataset.flatten_tensors(target_dataset.get_tensors(index))\n",
    "\n",
    "\n",
    "def build_data_pipeline(datasets, sample_size=None):\n",
    "    if sample_size is None:\n",
    "        sample_size = [1.0] * len(datasets)\n",
    "    assert len(sample_size) == len(\n",
    "        datasets\n",
    "    ), \"Illegal array of sample sizes provided. Number of sample size does not match number of datasets\"\n",
    "    file_pointers = [\n",
    "        x.extract_file_pointers().sample(frac=s, random_state=999)\n",
    "        for x, s in zip(datasets, sample_size)\n",
    "    ]\n",
    "    all_file_pointers = pd.concat(file_pointers).sample(frac=1, random_state=999)\n",
    "    print(f\"Total samples : {len(all_file_pointers)}\")\n",
    "\n",
    "    all_file_pointers[\"dataset_name\"] = all_file_pointers[\"dataset_name\"].apply(\n",
    "        lambda x: DATASETS_NAME.index(x)\n",
    "    )\n",
    "\n",
    "    final_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            all_file_pointers[\"metadata_index\"].tolist(),\n",
    "            all_file_pointers[\"dataset_name\"].tolist(),\n",
    "        )\n",
    "    )\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f30aff-2b6e-48f9-9e2e-977224538d11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples : 4826\n",
      "Total samples : 101000\n",
      "Total samples : 271407\n"
     ]
    }
   ],
   "source": [
    "recipes5k_dataset = build_data_pipeline([RECIPES5K])\n",
    "food101_dataset = build_data_pipeline([FOOD101])\n",
    "nutrition5k_dataset = build_data_pipeline([NUTRITION5K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2910985d-06c6-47bb-979d-bedb7b7d1a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(), dtype=int32, numpy=2344>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=0>),\n",
       " (<tf.Tensor: shape=(), dtype=int32, numpy=4556>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=0>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(recipes5k_dataset.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3f017-46fc-4bdc-8eda-8d7aa69b4097",
   "metadata": {},
   "source": [
    "## Serializing Data Pipeline to TFRecord with TFDS Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8175168-8a97-4ea6-a457-279dbf582c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_DICTIONARY = tfds.features.FeaturesDict(\n",
    "    {\n",
    "        \"image_raw\": tfds.features.Image(\n",
    "            shape=(224, 224, 3), doc=\"Raw bytes of food images encoded with tf.io\"\n",
    "        ),\n",
    "        \"category\": tfds.features.Scalar(dtype=tf.string, doc=\"Category label\"),\n",
    "        \"calorie\": tfds.features.Scalar(\n",
    "            dtype=tf.float32, doc=\"Calorie of the food per gram\"\n",
    "        ),\n",
    "        \"carbs\": tfds.features.Scalar(\n",
    "            dtype=tf.float32, doc=\"Carbs of the food per gram\"\n",
    "        ),\n",
    "        \"protein\": tfds.features.Scalar(\n",
    "            dtype=tf.float32, doc=\"Protein of the food per gram\"\n",
    "        ),\n",
    "        \"fat\": tfds.features.Scalar(dtype=tf.float32, doc=\"Fat of the food per gram\"),\n",
    "        \"ingredients\": tfds.features.Scalar(\n",
    "            dtype=tf.string, doc=\"Ingredients of food separated with comma\"\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2330f2-ee56-40d7-9404-a4025452bbcf",
   "metadata": {},
   "source": [
    "### Shard and write to TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5224e67b-b845-4493-9eee-1a39633d31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_and_write(dataset, num_shards, path, dataset_name):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        path.mkdir()\n",
    "\n",
    "    sharded_template_generator = tfds.core.ShardedFileTemplate(\n",
    "        data_dir=path.as_posix(),\n",
    "        template=\"{DATASET}-{SPLIT}-{SHARD_X_OF_Y}.{FILEFORMAT}\",\n",
    "        dataset_name=dataset_name,\n",
    "        filetype_suffix=\"tfrecord\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "    shard_length = []\n",
    "    sharded_filepaths = sharded_template_generator.sharded_filepaths(num_shards)\n",
    "    for i in range(num_shards):\n",
    "        current_shard = dataset.shard(num_shards, i)\n",
    "        with tf.io.TFRecordWriter(sharded_filepaths[i].as_posix()) as writer:\n",
    "            length = 0\n",
    "            for record in current_shard.as_numpy_iterator():\n",
    "                data = get_file_data(record[0], record[1])\n",
    "                example = {\n",
    "                    \"image_raw\": data[0],\n",
    "                    \"category\": data[1],\n",
    "                    \"calorie\": data[2],\n",
    "                    \"carbs\": data[3],\n",
    "                    \"protein\": data[4],\n",
    "                    \"fat\": data[5],\n",
    "                    \"ingredients\": data[6],\n",
    "                }\n",
    "                example_bytes = FEATURE_DICTIONARY.serialize_example(example)\n",
    "                writer.write(example_bytes)\n",
    "                length += 1\n",
    "            shard_length.append(length)\n",
    "    split_info = [\n",
    "        tfds.core.SplitInfo(\n",
    "            name=\"train\",\n",
    "            shard_lengths=shard_length,\n",
    "            num_bytes=0,\n",
    "            filename_template=sharded_template_generator,\n",
    "        )\n",
    "    ]\n",
    "    tfds.folder_dataset.write_metadata(\n",
    "        data_dir=path.as_posix(),\n",
    "        features=FEATURE_DICTIONARY,\n",
    "        filename_template=\"{DATASET}-{SPLIT}-{SHARD_X_OF_Y}.{FILEFORMAT}\",\n",
    "        split_infos=split_info,\n",
    "    )\n",
    "    return shard_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3fbc0c2-c3ba-4979-81fc-4f7fa20d4f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shard_length = shard_and_write(\n",
    "    recipes5k_dataset,\n",
    "    10,\n",
    "    f\"../Food Datasets/final-dataset/tfrecord/{RECIPES5K.name}/1.0.0\",\n",
    "    RECIPES5K.name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a37df5-2dc7-454f-b78c-c9c5cc773843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata written. Testing by reading first example. Set check_data=False to skip.\n"
     ]
    }
   ],
   "source": [
    "shard_length2 = shard_and_write(\n",
    "    food101_dataset,\n",
    "    30,\n",
    "    f\"../Food Datasets/final-dataset/tfrecord/{FOOD101.name}/1.1.0\",\n",
    "    FOOD101.name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580157ab-38c1-4725-82ce-2c15da26b548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata written. Testing by reading first example. Set check_data=False to skip.\n"
     ]
    }
   ],
   "source": [
    "shard_length3 = shard_and_write(\n",
    "    nutrition5k_dataset,\n",
    "    20,\n",
    "    f\"../Food Datasets/final-dataset/tfrecord/{NUTRITION5K.name}/1.0.0\",\n",
    "    NUTRITION5K.name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d605606-2412-45d8-a609-1818a9db999e",
   "metadata": {},
   "source": [
    "# Exported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "115d8ff3-8289-46a2-b34c-8c83e8d54cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED = {\"datasets\": DATASETS, \"one_hot_encoder\": ONE_HOT_ENCODER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd129b6-b1b5-489f-84f6-18a10968bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED = SimpleNamespace(**EXPORTED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64d4ec8-2e89-486f-a010-943e7ad2bbaa",
   "metadata": {},
   "source": [
    "## Export Encoded Categories and Ingredients for Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "207fa0a3-5c91-4898-9dfe-9d68b6eeee07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(\n",
    "    ONE_HOT_ENCODER.all_food_categories_integer_encoded,\n",
    "    open(\"./encoded_food_categories.json\", \"w\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a528076-317c-4502-8d54-943a69f8ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(\n",
    "    ONE_HOT_ENCODER.all_ingredients_integer_encoded,\n",
    "    open(\"./encoded_ingredients.json\", \"w\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62107dc2-f723-40ea-93df-a4aba5722e6f",
   "metadata": {},
   "source": [
    "# Data Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f66f8-15aa-4952-85d1-a6083a5ec336",
   "metadata": {},
   "source": [
    "## Categories Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "677c827d-1357-48fb-89bf-e794ad216c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of ingredients : 102\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of ingredients : {len(ONE_HOT_ENCODER.all_food_categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f94e841d-50aa-4bab-b473-311610d1fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([RECIPES5K.metadata, FOOD101.metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36a7885f-054f-4ab4-8b9b-1c0ec935aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_categories = df.groupby(\"Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8abb57ec-91eb-48b2-a8b9-755ac4115ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest number of images per category : 1008\n",
      "The highest number of images per category : 1050\n",
      "The average number of images per category : 1047\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"The lowest number of images per category : {df_grouped_categories.size().min()}\"\n",
    ")\n",
    "print(\n",
    "    f\"The highest number of images per category : {df_grouped_categories.size().max()}\"\n",
    ")\n",
    "total_imgs = df_grouped_categories.size().sum()\n",
    "total_category = len(df_grouped_categories)\n",
    "\n",
    "print(f\"The average number of images per category : {total_imgs//total_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4c810-077a-4b08-be17-84ffc3a75042",
   "metadata": {},
   "source": [
    "## Ingredients Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d4c10755-28a1-465d-832d-5b94c08f6623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of ingredients : 1037\n"
     ]
    }
   ],
   "source": [
    "print(f\"The total number of ingredients : {len(ONE_HOT_ENCODER.all_ingredients)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e161435d-f7ee-4974-8083-7ae7fb13fcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of ingredients for each dish (recipes5k + food101) : 9\n"
     ]
    }
   ],
   "source": [
    "average = 0\n",
    "for ing in df[\"Ingredients\"]:\n",
    "    count_ing = len(ing.split(\",\"))\n",
    "    average += count_ing\n",
    "average = average // len(df)\n",
    "print(\n",
    "    f\"The average number of ingredients for each dish (recipes5k + food101) : {average}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de469fa6-6107-4949-8825-c6f187054d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nutrition5k = NUTRITION5K.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "80c0cf3f-b6a9-4990-aa90-ba546d2be2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of ingredients for each dish (nutrition5k) : 5\n"
     ]
    }
   ],
   "source": [
    "average = 0\n",
    "for ing in df_nutrition5k[\"Ingredients\"]:\n",
    "    count_ing = len(ing.split(\",\"))\n",
    "    average += count_ing\n",
    "average = average // len(df_nutrition5k)\n",
    "print(f\"The average number of ingredients for each dish (nutrition5k) : {average}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07aee62-b540-437e-8ce1-e4843d6a4e96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6492e0f-92b3-47f8-a9a5-fe1d5d56ec39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RECIPES5K.get_tensors(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03d56e-53b7-4315-8fc8-5519b9cde9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_file_data(1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PY38-TF28-GPU",
   "language": "python",
   "name": "py38-tf28-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
